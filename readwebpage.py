

import requests
from bs4 import BeautifulSoup
def url_check(u):
    url = u
    resp = requests.get(url)

    if resp.status_code == 200:
        print("Successfully opened the web page")
        soup = BeautifulSoup(resp.text, 'html.parser')
        res=comment_check(str(soup.encode("utf-8")))
        return res
    else:
        print("Error")
        res=comment_check("no content fetched because of n/w problem")
        return res
##
##data=ms_jobs()

import numpy as np
import pandas as pd
import re
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

import nltk
from nltk.corpus import stopwords

import pickle

# Clean Tweets
def comment_check(com):
    import datetime
    import nltk
    import pandas as pd
    import numpy as np
    import time
    import re
    import sys
    import pickle
    def get_word_features(wordlist):
        wordlist = nltk.FreqDist(wordlist)
        word_features = wordlist.keys()
        return word_features

    def get_words_in_sentences(sentences):
        all_words = []
        for (words, sentiment) in sentences:
            all_words.extend(words)
        return all_words
##    def test_post(com):
    ##    if sys.version_info[0] == 3:
    ##        xrange = range
    print ("com",com)
    print(type(com))
    train = pd.read_csv("dataset.csv", header=0,delimiter=",", quoting=1, quotechar='"',encoding = "ISO-8859-1", engine='python')
    num_reviews = train["tags"].size
    print ("num_reviews",num_reviews)
    sentences = []

    for i in range(0,num_reviews):
        print(i,"ii.............",train["tags"][i])
        # Convert www.* or https?://* to URL
        sente = re.sub('((www\.[^\s]+)|(https?://[^\s]+))', '', train["tags"][i])
        # Convert @username to AT_USER
        sente = re.sub('@[^\s]+', '', sente)
        # Remove additional white spaces
        sente = re.sub('[\s]+', ' ', sente)
        # Replace #word with word
        sente = re.sub(r'#([^\s]+)', r'\1', sente)
        # trim
        sente = sente.strip('\'"')
        words_filtereds = [e.lower() for e in sente.split() if len(e) >= 3]
        sentences.append((words_filtereds, train["result"][i]))
    print(".................... COMPLETED.................................")
    word_features = get_word_features(get_words_in_sentences(sentences))

    def extract_features(document):
        document_words = set(document)
        features = {}
        for word in word_features:
            features['contains(%s)' % word] = (word in document_words)
        return features

    sents = com
    desicion = ""
    attc = ""
    if len(sents) > 1:
        
            #every char except alphabets is replaced
            sente=re.sub('[^a-z\s]+',' ',sente,flags=re.IGNORECASE)
            # Convert to lower case
            # sente = sente_tests.lower()
            sente = re.sub('((www\.[^\s]+)|(https?://[^\s]+))', '', com)
            sente = re.sub('@[^\s]+', '', sente)
            # Remove additional white spaces
            sente = re.sub('[\s]+', ' ', sente)
            # Replace #word with word
            sente = re.sub(r'#([^\s]+)', r'\1', sente)
            # trim
            sente = sente.strip('\'"')
            f = open("myclass.pickle", 'rb')
            classi = pickle.load(f)
            emot = classi.classify(extract_features(sente.split()))
            print (emot)
            desicion = emot
            dic = com + "=" + emot + "\n"

    desicion = desicion.strip()
    print ("..............................Desicion", desicion)
    return desicion
